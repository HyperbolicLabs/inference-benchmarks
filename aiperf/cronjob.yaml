apiVersion: batch/v1
kind: CronJob
metadata:
  name: aiperf-benchmark
  namespace: inference-benchmark
  labels:
    app: aiperf
    component: benchmark
spec:
  # Run every 9 minutes with 8-minute benchmarks for continuous load
  # This ensures no gaps while preventing overlap (1 minute buffer)
  schedule: "*/9 * * * *"
  # Prevent multiple jobs from running simultaneously
  # If a job is still running when the next schedule time arrives, skip it
  concurrencyPolicy: Forbid
  # Keep only recent job history to avoid resource accumulation
  # 5 successful jobs = ~50 minutes of history
  # 3 failed jobs = enough to debug issues
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: aiperf
            component: benchmark
        spec:
          restartPolicy: OnFailure
          imagePullSecrets:
          - name: ghcr-image-pull-secret
          containers:
          - name: aiperf
            # AIPerf benchmarking image from GitHub Container Registry
            # Build and push with: cd scripts/aiperf && make build-push
            image: ghcr.io/hyperboliclabs/aiperf:latest
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - |
              set -e
              echo "=== Running AIPerf Benchmark ==="
              python3 /scripts/benchmark.py
              
              echo "=== Benchmark Complete ==="
              ls -lh /tmp/results/ || echo "No results directory"
            env:
            - name: MODEL_NAME
              value: "Qwen/Qwen3-VL-32B-Thinking"
            - name: ENDPOINT_URL
              value: "https://inference.hyperbolic.ai"
            - name: ENDPOINT_TYPE
              value: "chat"
            # Cloudflare Access credentials (required for authentication)
            # Update these values with your actual Cloudflare Access service token credentials
            - name: CF_ACCESS_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: cloudflare-access-credentials
                  key: client-id
                  optional: true
            - name: CF_ACCESS_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: cloudflare-access-credentials
                  key: client-secret
                  optional: true
            # Concurrency: 20 provides production-like load testing
            # This is optimal for continuous load testing without overwhelming the system
            # For stress testing, use 50+
            - name: CONCURRENCY
              value: "20"
            # Use duration-based benchmarking (8 minutes) for sustained load
            # With 10-minute schedule, this creates 2-minute overlaps for continuous load
            - name: BENCHMARK_DURATION
              value: "480"
            - name: BENCHMARK_GRACE_PERIOD
              value: "30"
            # Request timeout: 60 seconds per request (prevents hanging)
            - name: REQUEST_TIMEOUT
              value: "60"
            # Output token limit: Mean of 50 tokens per response for consistent benchmarking
            # This prevents unbounded responses and makes results more predictable
            - name: OUTPUT_TOKENS_MEAN
              value: "50"
            # Keep request_count as a safety limit (will be ignored if benchmark_duration is set)
            - name: REQUEST_COUNT
              value: "10000"
            - name: STREAMING
              value: "true"
            - name: OUTPUT_DIR
              value: "/tmp/results"
            # Datadog API key for metrics export (optional)
            - name: DD_API_KEY
              valueFrom:
                secretKeyRef:
                  name: datadog-api-key
                  key: api-key
                  optional: true
            volumeMounts:
            - name: results
              mountPath: /tmp/results
            resources:
              requests:
                memory: "1Gi"      # Reduced: AIPerf is I/O bound, minimal memory needed
                cpu: "200m"        # Reduced: Mostly waiting for HTTP responses
              limits:
                memory: "2Gi"      # Reduced: Sufficient headroom for parsing
                cpu: "1000m"       # Reduced: Burst capacity for metric processing
          volumes:
          - name: results
            persistentVolumeClaim:
              claimName: aiperf-results
              # Optional: Use emptyDir for ephemeral results
              # emptyDir: {}
